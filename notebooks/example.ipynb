{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example workflow using the utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple classifier for MNIST using the sacred trainer utils to make our job easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run\n",
    "filename = 'example_mnist.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import *\n",
    "from pytorch_utils.updaters import *\n",
    "import pytorch_utils.sacred_trainer as st\n",
    "import torch.nn.functional as F\n",
    "from sacred import Experiment\n",
    "from sacred.observers import FileStorageObserver\n",
    "from visdom_observer.visdom_observer import VisdomObserver\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "IPY=True\n",
    "try:\n",
    "    get_ipython()\n",
    "except:\n",
    "    IPY=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The sacred experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "ex = Experiment('mnistclassifier_example', interactive=IPY)\n",
    "save_dir = 'MnistClassifier'\n",
    "ex.observers.append(VisdomObserver())\n",
    "ex.observers.append(FileStorageObserver.create(save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the dataset and split into train, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def dataset_config():\n",
    "    val_split=0.1\n",
    "    batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def make_dataloaders(val_split, batch_size):\n",
    "    \n",
    "    # get dataset\n",
    "    total_train_mnist = MNIST('Z:/MNIST',download=True,transform=ToTensor())\n",
    "    test_mnist = MNIST('Z:/MNIST',train=False,transform=ToTensor())\n",
    "    \n",
    "    training_number = len(total_train_mnist)\n",
    "\n",
    "    train_num = int(training_number*(1-val_split))\n",
    "    val_num = int(training_number*val_split)\n",
    "\n",
    "    train_mnist, val_mnist = torch.utils.data.dataset.random_split(total_train_mnist, \n",
    "                    [train_num, val_num])\n",
    "    \n",
    "    \n",
    "    # split into data loaders\n",
    "    train_loader = DataLoader(train_mnist, batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_mnist, batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(test_mnist, batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a bunch of modules and integrate them into a final model. Normally, there would be a heirarchy of many different modules, but for this simple example, we can make do with a single module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def model_config():\n",
    "    hidden_size = 400 # hidden units in hidden layer\n",
    "    output_size = 10 # number of output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "class MnistClassifier(nn.Module):\n",
    "    \"\"\"A simple two layer fully connected neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(MnistClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(28*28, hidden_size)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.view(-1,28*28)\n",
    "        \n",
    "        out = F.relu(self.hidden_layer(input))\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.capture\n",
    "def make_model(hidden_size, output_size):\n",
    "    return MnistClassifier(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an optimizer for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def optimizer_config():\n",
    "    lr=0.001 # learning rate\n",
    "    weight_decay=0 # l2 regularization factor\n",
    "    \n",
    "@ex.capture\n",
    "def make_optimizer(model, lr, weight_decay):\n",
    "    return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train on batch function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to write a function with the following signature:  \n",
    "\n",
    "`trainOnBatch(model, batch, optimizer) -> tuple of metrics`  \n",
    "\n",
    "This function is supposed to perform one training step on the model using the given optimizer and batch. Note that the format in which the data comes in through the batch can be specified by us, as we build our own `Dataset` classes and `DataLoader`s with custom collate functions for each application. The trainer utility only requires that this function take in the three arguments as shown above and return a tuple of scalar metrics.  \n",
    "\n",
    "This function is called on each batch in the training `DataLoader` and the metrics it returns are updated each batch according to a set of running metric updaters. The final running metrics logged as scalars by the sacred experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def classification_train_on_batch(model,batch,optimizer):\n",
    "    \n",
    "    # batch is tuple containing (tensor of images, tensor of labels)\n",
    "    outputs = model(batch[0]) # forward pass\n",
    "    \n",
    "    # compute loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs,batch[1])\n",
    "    \n",
    "    # backward pass and weight update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # compute and return metrics\n",
    "    loss = loss.detach().cpu().numpy()\n",
    "    acc = st.accuracy(outputs, batch[1])\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The callback function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function which the trainer util uses is the `callback` function. It is an optional function, and its main purpose is for validation. Note that it can be used for other purposes like learning rate scheduling as well. The basic signature required is as follows:  \n",
    "\n",
    "`callback(model, val_loader) -> tuple of callback metrics`\n",
    "\n",
    "The `callback` receives the model instance and the validation `DataLoader`. It is expected to return a tuple of scalar callback metrics. This function is called at the end of every epoch of training, and the metrics it returns are logged as scalars by the sacred experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def classification_callback(model, val_loader):\n",
    "    with torch.no_grad(): # dont compute gradients\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        model.eval() # eval mode\n",
    "        \n",
    "        batches = len(val_loader)\n",
    "        loss=0\n",
    "        acc=0\n",
    "        for batch in val_loader:\n",
    "            outputs = model(batch[0])\n",
    "            loss += criterion(outputs,batch[1])\n",
    "            acc += st.accuracy(outputs,batch[1])\n",
    "        \n",
    "        # find average loss and accuracy over whole vaildation set\n",
    "        loss/= batches\n",
    "        acc /= batches\n",
    "        \n",
    "        model.train() # go back to train mode\n",
    "        \n",
    "        # return metrics\n",
    "        return loss.cpu().numpy(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.config\n",
    "def train_config():\n",
    "    epochs=10\n",
    "    save_every=1 # epoch interval with which to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - mnistclassifier_example - Running command 'print_config'\n",
      "INFO - mnistclassifier_example - Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (modified, added, typechanged, doc):\n",
      "  batch_size = 32\n",
      "  epochs = 10\n",
      "  hidden_size = 400                  # hidden units in hidden layer\n",
      "  lr = 0.001                         # learning rate\n",
      "  output_size = 10                   # number of output labels\n",
      "  save_every = 1                     # epoch interval with which to save models\n",
      "  seed = 311952711                   # the random seed for this experiment\n",
      "  val_split = 0.1\n",
      "  weight_decay = 0                   # l2 regularization factor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - mnistclassifier_example - Completed after 0:00:00\n"
     ]
    }
   ],
   "source": [
    "r = ex.run('print_config')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have defined all the functions we need for training our model. Let us write the sacred command function to run the training. We'll call all the above methods, and then use the `sacred_trainer.loop` function to make our lives easier. We need to pass in the defined dataloaders, functions, model and optimizer, as well as the config variables we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the loop command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "@ex.command\n",
    "def train(_run):\n",
    "    \n",
    "    train_loader, val_loader, test_loader = make_dataloaders()\n",
    "    model = make_model()\n",
    "    optimizer = make_optimizer(model)\n",
    "    \n",
    "    st.loop(\n",
    "        _run=_run,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        save_dir=save_dir,\n",
    "        trainOnBatch=classification_train_on_batch,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        callback=classification_callback,\n",
    "        **_run.config,\n",
    "        **st.CLASSIFICATION,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a -dr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ex.run_commandline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run it in the notebook or as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - mnistclassifier_example - Running command 'train'\n",
      "INFO - visdom - Visdom successfully connected to server\n",
      "INFO - mnistclassifier_example - Started run with ID \"33\"\n",
      "Epoch: 1: 100%|█████████████████████████████████████████████| 1688/1688 [00:16<00:00, 103.02it/s, acc=92.9, loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.117990 val_acc=96.575798\n",
      "MnistClassifier\\33\\epoch001_23-08_2358_val_loss0.1180_val_acc96.5758.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|████████████████████████████████████████████| 1688/1688 [00:14<00:00, 117.16it/s, acc=97.1, loss=0.0985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.087185 val_acc=97.523271\n",
      "MnistClassifier\\33\\epoch002_23-08_2358_val_loss0.0872_val_acc97.5233.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████████████████████████████████████████| 1688/1688 [00:14<00:00, 115.97it/s, acc=98, loss=0.0645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.089751 val_acc=97.257314\n",
      "MnistClassifier\\33\\epoch003_23-08_2359_val_loss0.0898_val_acc97.2573.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|████████████████████████████████████████████| 1688/1688 [00:14<00:00, 120.47it/s, acc=98.5, loss=0.0468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.077829 val_acc=97.656250\n",
      "MnistClassifier\\33\\epoch004_23-08_2359_val_loss0.0778_val_acc97.6562.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████████████████████████████████████████| 1688/1688 [00:14<00:00, 117.69it/s, acc=99, loss=0.0327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.078846 val_acc=97.789229\n",
      "MnistClassifier\\33\\epoch005_23-08_2359_val_loss0.0788_val_acc97.7892.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|████████████████████████████████████████████| 1688/1688 [00:16<00:00, 101.17it/s, acc=99.2, loss=0.0254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.073868 val_acc=97.905585\n",
      "MnistClassifier\\33\\epoch006_23-08_2359_val_loss0.0739_val_acc97.9056.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|████████████████████████████████████████████| 1688/1688 [00:15<00:00, 108.12it/s, acc=99.4, loss=0.0194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.068194 val_acc=97.822473\n",
      "MnistClassifier\\33\\epoch007_24-08_0000_val_loss0.0682_val_acc97.8225.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|████████████████████████████████████████████| 1688/1688 [00:14<00:00, 115.17it/s, acc=99.5, loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.084216 val_acc=97.905585\n",
      "MnistClassifier\\33\\epoch008_24-08_0000_val_loss0.0842_val_acc97.9056.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|████████████████████████████████████████████| 1688/1688 [00:15<00:00, 111.94it/s, acc=99.6, loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.083135 val_acc=97.855718\n",
      "MnistClassifier\\33\\epoch009_24-08_0000_val_loss0.0831_val_acc97.8557.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|████████████████████████████████████████████| 1688/1688 [00:16<00:00, 99.33it/s, acc=99.7, loss=0.0106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback metrics: val_loss=0.092859 val_acc=97.606383\n",
      "MnistClassifier\\33\\epoch010_24-08_0000_val_loss0.0929_val_acc97.6064.statedict.pkl\n",
      "MnistClassifier\\33\\epoch010_24-08_0000_val_loss0.0929_val_acc97.6064.statedict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - mnistclassifier_example - Completed after 0:02:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sacred.run.Run at 0x2937d693550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.run('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py36]",
   "language": "python",
   "name": "conda-env-pytorch_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
