{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience module for pytorch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run\n",
    "filename = '../package/pytorch_utils/sacred_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Convenience module for pytorch training and visualization. Uses sacred to \n",
    "log experiments and visdom for visualization.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import *\n",
    "import os, time\n",
    "import traceback\n",
    "from pytorch_utils.updaters import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def getTimeName():\n",
    "    \"\"\"Return the current time in format <day>-<month>_<hour><minute> for use in filenames.\"\"\"\n",
    "    from datetime import datetime\n",
    "    t = datetime.now()\n",
    "    return \"{:02d}-{:02d}_{:02d}{:02d}\".format(t.day,t.month,t.hour,t.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def accuracy(scores, labels):\n",
    "    \"\"\"Return accuracy percentage. Assumes scores are in dim -1.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        total = scores.size()[0]\n",
    "        pred = torch.argmax(scores, dim=-1)\n",
    "        correct = (pred == labels).sum().cpu().numpy().astype('float32')\n",
    "        acc = correct/total * 100\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def save_model(model, epoch, directory, metrics):\n",
    "    \"\"\"Save the state dict of the model in the directory, \n",
    "    with the save name metrics at the given epoch.\n",
    "\n",
    "    epoch: epoch number(<= 3 digits)\n",
    "    directory: where to save statedict\n",
    "    metrics: dictionary of metrics to append to save filename\n",
    "\n",
    "    Returns the save file name\n",
    "    \"\"\"\n",
    "    # save state dict\n",
    "    filename = f\"epoch{epoch:03d}_{getTimeName()}_\"\n",
    "\n",
    "    postfix = \"_\".join([f\"{name}{val:0.4f}\" for name,val in metrics.items()])\n",
    "\n",
    "    filename = os.path.join(directory, filename+postfix+\".statedict.pkl\")\n",
    "    print(filename)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def loop(_run,\n",
    "     model,\n",
    "     batch_metric_names,\n",
    "     train_loader, \n",
    "     trainOnBatch,\n",
    "     optimizer, \n",
    "\n",
    "     updaters, \n",
    "     save_dir, \n",
    "\n",
    "     callback=None,\n",
    "     callback_metric_names=[],\n",
    "     val_loader=None, \n",
    "\n",
    "     epochs=10, \n",
    "     save_every=1, \n",
    "     start_epoch=1,\n",
    "     **kwargs,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    -------------------\n",
    "     _run: Sacred run instance\n",
    "\n",
    "    model: Model instance to be trained\n",
    "    \n",
    "    trainOnBatch: Train on batch function with the following signature:\n",
    "        trainOnBatch(model, batch, optimizer) -> tuple of batch metrics\n",
    "    batch_metric_names: Names of the batch metrics returned by trainOnBatch\n",
    "    \n",
    "    train_loader: DataLoader which yields batches of training data\n",
    "    optimizer: Optimizer instance which is passed to trainOnBatch \n",
    "    updaters: List of running metric updaters which aggregate batch metrics per epoch.\n",
    "        They should be generator functions which return the running value when \n",
    "        .send is called with a batch value.\n",
    "    save_dir: Top level directory in which to save configs, checkpoints and metrics \n",
    "    of each run\n",
    "      \n",
    "    callback=None : Optional callback function, should have the following signature:\n",
    "        callback(model, val_loader) -> tuple of callback metrics\n",
    "        Usually used for calculating validation metrics.\n",
    "    callback_metric_names=[]: Names of the above returned callback metrics\n",
    "    val_loader=None: DataLoader for validation data.\n",
    "    \"\"\"\n",
    "    run_dir = os.path.join(save_dir, str(_run._id))\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        for e in range(start_epoch,start_epoch+epochs,1):\n",
    "\n",
    "            i=0\n",
    "\n",
    "            upds = [updater() for updater in updaters]\n",
    "            [next(u) for u in upds]\n",
    "\n",
    "            t = tqdm(train_loader, desc=f'Epoch: {e}')  \n",
    "            for i,batch in enumerate(t):\n",
    "                # Perform train step\n",
    "                batch_metrics = trainOnBatch(model, batch, optimizer)\n",
    "\n",
    "                # Update running metrics\n",
    "                batch_metrics = [upds[i].send(b_metric) \n",
    "                                    for i,b_metric in enumerate(batch_metrics)]\n",
    "\n",
    "                t.set_postfix(**dict(zip(batch_metric_names, batch_metrics)))\n",
    "\n",
    "            # execute callback\n",
    "            callback_metrics = ()\n",
    "            if callback != None:\n",
    "                callback_metrics = callback(model, val_loader=val_loader)\n",
    "\n",
    "            mets = dict(zip(callback_metric_names, callback_metrics))\n",
    "\n",
    "            if len(callback_metrics)!=0:\n",
    "                cb_info = \"Callback metrics: \" + \" \".join([f\"{name}={val:.6f}\" \n",
    "                                for name,val in mets.items()])\n",
    "                print(cb_info)\n",
    "\n",
    "            # log metrics\n",
    "            for name,val in zip(batch_metric_names, batch_metrics):\n",
    "                _run.log_scalar(name, val, e)    \n",
    "            for name,val in zip(callback_metric_names, callback_metrics):\n",
    "                _run.log_scalar(name, val, e)\n",
    "\n",
    "            # Checkpointing\n",
    "            if e%(save_every)==0:\n",
    "                fname = save_model(model, e, run_dir, mets)\n",
    "                _run.add_artifact(fname)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(\"Exception occured, saving optimizer and model\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "\n",
    "        # save optimizer state\n",
    "        fname = os.path.join(\n",
    "                    run_dir,                \n",
    "                    f\"optimizer_state_epoch{e:03d}.statedict.pkl\"\n",
    "                )\n",
    "        torch.save(optimizer.state_dict(),fname)\n",
    "        _run.add_artifact(fname)\n",
    "\n",
    "        # save model dict\n",
    "        fname = save_model(model, e, run_dir, mets)\n",
    "        _run.add_artifact(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "CLASSIFICATION = dict(\n",
    "    batch_metric_names=['loss', 'acc'],\n",
    "    callback_metric_names=['val_loss', 'val_acc'],\n",
    "    updaters=[averager, averager],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py36]",
   "language": "python",
   "name": "conda-env-pytorch_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
