{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful modules for RNNs in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run wrappedLSTM.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module which wraps an input and output module around an LSTM.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped LSTM with input and output modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run wrappedLSTM.py -a\n",
    "\n",
    "\n",
    "class WrappedLSTM(nn.Module):\n",
    "    def __init__(self, lstm_input_size, lstm_hidden_size, input_module=None, output_module=None):\n",
    "        \"\"\"\n",
    "        lstm_input_size should equal input_module output size\n",
    "        lstm_hidden_size should equal output_module input size\n",
    "        \"\"\"\n",
    "        super(WrappedLSTM, self).__init__()\n",
    "        \n",
    "        self.input_module = input_module\n",
    "        self.output_module = output_module\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_size,hidden_size=lstm_hidden_size)\n",
    "        \n",
    "    def forward(self,hidden, *packed_input):\n",
    "        \"\"\"\n",
    "        Applies input module to data in packed_inputs, \n",
    "        then applies the LSTM layers,\n",
    "        Applied output module to output data of rnn,\n",
    "        \n",
    "        Returns packed output sequence and final hidden state of LSTM.\n",
    "        \"\"\"\n",
    "        batch_sizes = packed_input[0].batch_sizes\n",
    "        \n",
    "        if self.input_module != None:\n",
    "            rnn_input = self.input_module(*[p.data for p in packed_input])\n",
    "            rnn_input = utils.PackedSequence(rnn_input,batch_sizes)\n",
    "        else:\n",
    "            rnn_input = packed_input[0]\n",
    "            \n",
    "        rnn_output, hidden = self.lstm(rnn_input,hidden)\n",
    "        \n",
    "        if self.output_module != None:\n",
    "            output = self.output_module(rnn_output.data)\n",
    "            output = utils.PackedSequence(output,batch_sizes)\n",
    "        else:\n",
    "            output = rnn_output\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMmodel(nn.Module):\n",
    "    def __init__(self,num_embeddings, embedding_size, lstm_hidden_size):\n",
    "        \"\"\"\n",
    "        num_embeddings sized input and outputs.\n",
    "        lstm output interfaced to final output through Dense layer.\n",
    "        \"\"\"\n",
    "        super(MyLSTMmodel, self).__init__()\n",
    "        \n",
    "        embed = nn.Embedding(num_embeddings, embedding_size)\n",
    "        \n",
    "        # hidden to output\n",
    "        h2o = nn.Linear(lstm_hidden_size, num_embeddings)\n",
    "        \n",
    "        self.wrappedlstm = WrappedLSTM(embedding_size, lstm_hidden_size,embed, h2o)\n",
    "        \n",
    "        self.hidden_size = lstm_hidden_size\n",
    "        \n",
    "    def forward(self, packed_input, hidden):\n",
    "        packed_output, hidden = self.wrappedlstm(packed_input, hidden)\n",
    "        return packed_output, hidden\n",
    "    \n",
    "    def initHidden(self, num_seqs):\n",
    "        return torch.rand(1,num_seqs,self.hidden_size),torch.rand(1,num_seqs,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = MyLSTMmodel(10,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([0,4,1,1,2])\n",
    "b = torch.LongTensor([3,7,6,5])\n",
    "c = torch.LongTensor([8,1])\n",
    "sample_input = utils.pack_sequence([a,b,c])\n",
    "sample_hidden = lstm.initHidden(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(sample_hidden,sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1198,  0.2623, -0.1670,  0.1608, -0.0619, -0.0063, -0.1187,\n",
       "         -0.0892,  0.1493, -0.0194],\n",
       "        [-0.1502,  0.2648, -0.0497,  0.1750,  0.0225,  0.0755, -0.1130,\n",
       "         -0.1128,  0.1811, -0.1531],\n",
       "        [-0.0749,  0.2787, -0.0570,  0.1000,  0.0272,  0.0323, -0.0653,\n",
       "         -0.1151,  0.1706, -0.0051],\n",
       "        [-0.1785,  0.1828, -0.1452,  0.0981, -0.0155, -0.0578, -0.1371,\n",
       "         -0.0514,  0.2532, -0.0751],\n",
       "        [-0.1527,  0.2111, -0.1164,  0.0885,  0.0797,  0.0234, -0.0679,\n",
       "         -0.0652,  0.1986,  0.0064],\n",
       "        [-0.0912,  0.2530, -0.0036,  0.0616,  0.1297,  0.0063, -0.0319,\n",
       "         -0.1574,  0.1323, -0.0517],\n",
       "        [-0.0825,  0.2068, -0.0488,  0.0304,  0.1356,  0.0468, -0.0999,\n",
       "         -0.1165,  0.1842, -0.1161],\n",
       "        [-0.0806,  0.2818, -0.1655, -0.1356,  0.1483, -0.0135, -0.0359,\n",
       "         -0.0588,  0.2090,  0.0693],\n",
       "        [-0.0647,  0.1890, -0.0069,  0.0352,  0.2219,  0.0822, -0.0694,\n",
       "         -0.0774,  0.0981, -0.1270],\n",
       "        [-0.0122,  0.1959, -0.0735, -0.1664,  0.0731,  0.0150, -0.0144,\n",
       "         -0.0418,  0.2322, -0.0203],\n",
       "        [ 0.0148, -0.0462, -0.0009, -0.1330,  0.1133,  0.0957, -0.0565,\n",
       "         -0.0098,  0.0647, -0.1988]]), batch_sizes=tensor([ 3,  3,  2,  2,  1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py36]",
   "language": "python",
   "name": "conda-env-pytorch_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
